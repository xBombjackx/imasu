import streamlit as st
import requests
import os
from PIL import Image
import sys

# Add the root project directory to the Python path
# This allows us to import from the 'app' module if ever needed,
# though for this simple UI, we'll use API calls.
# Note: When running with Docker, this becomes less critical.
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# --- Configuration ---
# This is the URL for your FastAPI backend.
# If running locally, it's typically this.
# If using Docker Compose, this would be 'http://orchestrator:8000/agent/generate'
API_URL = "http://127.0.0.1:8000/agent/generate"

# This should point to the output directory relative to the project root.
# Your FastAPI app saves images in `app/final_outputs`.
OUTPUT_DIR = "app/final_outputs"

# --- Streamlit UI ---
st.set_page_config(layout="wide")
st.title("ðŸŽ¨ Agentic Artwork Generator")
st.caption("A local, self-hosted agentic system.")

# Initialize chat history in session state
if "messages" not in st.session_state:
    st.session_state.messages = []
    # Add a welcoming message
    st.session_state.messages.append(
        {
            "role": "assistant",
            "content": "Hello! What masterpiece can I create for you today?",
        }
    )

# Display prior chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        # If an image path is stored with the message, display it
        if "image_path" in message and message["image_path"]:
            if os.path.exists(message["image_path"]):
                st.image(message["image_path"])
            else:
                st.error(f"Could not find image at path: {message['image_path']}")

# Accept user input
if prompt := st.chat_input("e.g., 'Create a photorealistic cat wearing a wizard hat'"):
    # Add user message to UI and history
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # Call the agent backend
    with st.spinner("The agent is reasoning and creating..."):
        try:
            response = requests.post(API_URL, json={"prompt": prompt})
            response.raise_for_status()  # Raises an HTTPError for bad responses (4xx or 5xx)

            agent_response = response.json().get("response", {})
            output_text = agent_response.get(
                "output", "Sorry, I couldn't generate a response."
            )
            image_filename = agent_response.get(
                "image_filename"
            )  # Get the filename from the API

            # Display agent response and image
            with st.chat_message("assistant"):
                st.markdown(output_text)
                image_full_path = None
                if image_filename:
                    # Construct the full path to the image
                    image_full_path = os.path.join(OUTPUT_DIR, image_filename)
                    if os.path.exists(image_full_path):
                        st.image(image_full_path, caption="Generated by the Agent")
                    else:
                        st.error(
                            f"Agent reported image '{image_filename}', but the file was not found in '{OUTPUT_DIR}'."
                        )

            # Add agent response to chat history, including the image path
            st.session_state.messages.append(
                {
                    "role": "assistant",
                    "content": output_text,
                    "image_path": image_full_path,
                }
            )

        except requests.exceptions.RequestException as e:
            st.error(
                f"Failed to connect to the agent backend at {API_URL}. Is it running? Error: {e}"
            )
        except Exception as e:
            st.error(f"An unexpected error occurred: {e}")
